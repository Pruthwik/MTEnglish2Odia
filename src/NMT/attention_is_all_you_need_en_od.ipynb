{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention is all you need en-od.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Bcf8ybPq28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, Iterator\n",
        "from torchtext.data import batch as ttbatch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEV5SKDLRaTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94iZ4SE9RJty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O7_l9hCR9vG",
        "colab_type": "code",
        "outputId": "3eb75c2b-1ef7-4f7a-9194-59e46fa5c2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXJWWLMaPvl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set random seed for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAmMj1wg3UBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        " \n",
        "def copyDirectory(src, dest):\n",
        "    try:\n",
        "        shutil.copytree(src, dest)\n",
        "    # Directories are the same\n",
        "    except shutil.Error as e:\n",
        "        print('Directory not copied. Error: %s' % e)\n",
        "    # Any error saying that the directory doesn't exist\n",
        "    except OSError as e:\n",
        "        print('Directory not copied. Error: %s' % e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7JZ7HcSCgl",
        "colab_type": "code",
        "outputId": "7733bdfb-d27e-438e-d20e-ec0bb713c634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_dir_drive = os.path.join('gdrive', 'My Drive', 'EN-OD translation', 'data')\n",
        "data_dir = 'data'\n",
        "copyDirectory(data_dir_drive, data_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory not copied. Error: [Errno 17] File exists: 'data'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7HuQ4DmWmIs",
        "colab_type": "code",
        "outputId": "6ed6f19b-1d8e-4cbd-e3c6-2a3c94aa821b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(os.listdir(data_dir))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bpe_od.model', 'corpus.od', 'bpe_od.vocab', 'dev.od', 'bpe_en.vocab', 'bpe_en.model', 'corpus.en', 'dev.en', 'test.od', 'test.en']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2KL-2uHYz65",
        "colab_type": "code",
        "outputId": "e305e015-c909-4814-b37b-3b186ea7899f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!head data/corpus.od"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ଭିଡିଓ ବିଜ୍ଞାପନ\r\n",
            "ଭିଡିଓ ବିଜ୍ଞାପନ\r\n",
            "ଗୋପନୀୟତା\r\n",
            "ଅପଠିତ\r\n",
            "ରିଚାର୍ଜ୍ ଦୋକାନ\r\n",
            "ପ୍ରିସେଟ୍‌ ଅଧ୍ୟୟନ\r\n",
            "ଡିଫଲ୍ଟ ବିଜ୍ଞାପନ\r\n",
            "ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ବିଭାଜନ\r\n",
            "ବିଜ୍ଞାପନ ପରିଚାଳକ\r\n",
            "ଅନୁକୂଳ କରାଇବା\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnkuiO2eZKPW",
        "colab_type": "code",
        "outputId": "deddaed6-4d14-4396-f5fa-219d546a3466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "for filename in sorted(os.listdir(data_dir)):\n",
        "  print(f\"{filename:10}: {sum(1 for line in open(os.path.join(data_dir, filename), 'r', encoding='utf-8'))}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-787e96aa7cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{filename:10}: {sum(1 for line in open(os.path.join(data_dir, filename), 'r', encoding='utf-8'))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-787e96aa7cb4>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{filename:10}: {sum(1 for line in open(os.path.join(data_dir, filename), 'r', encoding='utf-8'))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 57: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kci5fymVRhks",
        "colab_type": "code",
        "outputId": "ff618e35-2927-459f-97f9-17d19d99aa5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(f'--input={data_dir}/corpus.en --model_prefix={data_dir}/bpe_en --vocab_size=8000 --model_type=bpe')\n",
        "spm.SentencePieceTrainer.train(f'--input={data_dir}/corpus.od --model_prefix={data_dir}/bpe_od --vocab_size=8000 --model_type=bpe')\n",
        "sp_bpe_src = spm.SentencePieceProcessor()\n",
        "sp_bpe_trg = spm.SentencePieceProcessor()\n",
        "sp_bpe_src.load(f'{data_dir}/bpe_en.model')\n",
        "sp_bpe_trg.load(f'{data_dir}/bpe_od.model')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tds_DCazenP",
        "colab_type": "code",
        "outputId": "9bf0ed51-c2c1-454f-949f-dc3d4e6fd95d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(os.listdir(data_dir))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bpe_od.model', 'corpus.od', 'bpe_od.vocab', 'dev.od', 'bpe_en.vocab', 'bpe_en.model', 'corpus.en', 'dev.en', 'test.od', 'test.en']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R44OVVKMaCHv",
        "colab_type": "code",
        "outputId": "5c5e9f6b-1c70-4e51-9870-086ddbf9cc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l data/bpe_en.vocab"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 data/bpe_en.vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ev9R01mu3XE",
        "colab_type": "code",
        "outputId": "82f8029d-992b-4ec7-d0ab-c6db30996d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l data/bpe_od.vocab"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 data/bpe_od.vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAazHs4sRIe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize = sp_bpe_src.encode_as_pieces, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = False,\n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = sp_bpe_trg.encode_as_pieces,\n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>',\n",
        "            lower = False,\n",
        "            batch_first = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq14ubIZRLFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = TranslationDataset.splits(\n",
        "    path=data_dir,\n",
        "    exts=('.en', '.od'),\n",
        "    train='corpus',\n",
        "    validation='dev',\n",
        "    test='test',\n",
        "    fields=(SRC, TRG)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS_SzuFIIN8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in [train_data, valid_data, test_data]:\n",
        "  for sample in dataset:\n",
        "    assert len(sample.src) <= 1000 and len(sample.trg) <=1000, 'at least one sample has > 1000 tokens'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga9eew8wRNZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=1)\n",
        "TRG.build_vocab(train_data, min_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_QQl336abVm",
        "colab_type": "code",
        "outputId": "cb2e4c91-90a5-4ebb-934e-90de80a1180d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f'sizes of vocab:\\nen = {len(SRC.vocab)}\\nod = {len(TRG.vocab)}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sizes of vocab:\n",
            "en = 8352\n",
            "od = 8183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmccLbhRWKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLUL0tb76mLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# global variables\n",
        "global MAX_SRC_IN_BATCH, MAX_TRG_IN_BATCH\n",
        "\n",
        "\n",
        "# batch size function for MinPadIterator\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"\"\"Keep augmenting batch and calculate total number of tokens + padding.\"\"\"\n",
        "    global MAX_SRC_IN_BATCH, MAX_TRG_IN_BATCH\n",
        "    if count == 1:\n",
        "        MAX_SRC_IN_BATCH = 0\n",
        "        MAX_TRG_IN_BATCH = 0\n",
        "    MAX_SRC_IN_BATCH = max(MAX_SRC_IN_BATCH, len(new.src))\n",
        "    MAX_TRG_IN_BATCH = max(MAX_TRG_IN_BATCH, len(new.trg) + 2)\n",
        "    src_elements = count * MAX_SRC_IN_BATCH\n",
        "    trg_elements = count * MAX_TRG_IN_BATCH\n",
        "    return max(src_elements, trg_elements)\n",
        "\n",
        "\n",
        "# iterator to minimize padding\n",
        "class MinPadIterator(Iterator):\n",
        "    def create_batches(self):\n",
        "\n",
        "        def pool(d, random_shuffler):\n",
        "            for p in ttbatch(d, self.batch_size * 100):\n",
        "                p_batch = ttbatch(\n",
        "                    sorted(p, key=self.sort_key),\n",
        "                    self.batch_size, self.batch_size_fn)\n",
        "                for b in random_shuffler(list(p_batch)):\n",
        "                    yield b\n",
        "\n",
        "        self.batches = pool(self.data(), self.random_shuffler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asj16KOLRcJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8192\n",
        "\n",
        "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "#     (train_data, valid_data, test_data), \n",
        "#      batch_size = BATCH_SIZE,\n",
        "#      device = device)\n",
        "\n",
        "train_iterator = MinPadIterator(\n",
        "      train_data,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=True,\n",
        "      sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "      batch_size_fn=batch_size_fn,\n",
        "      device=device\n",
        "  )\n",
        "valid_iterator = MinPadIterator(\n",
        "      valid_data,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=False,\n",
        "      sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "      batch_size_fn=batch_size_fn,\n",
        "      device=device\n",
        "  )\n",
        "test_iterator = MinPadIterator(\n",
        "      test_data,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=False,\n",
        "      sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "      batch_size_fn=batch_size_fn,\n",
        "      device=device\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7YC9keZRs4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 encoder_layer, \n",
        "                 self_attention_layer, \n",
        "                 positionwise_feedforward_layer, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, \n",
        "                                                   n_heads, \n",
        "                                                   pf_dim, \n",
        "                                                   self_attention_layer, \n",
        "                                                   positionwise_feedforward_layer, \n",
        "                                                   dropout, \n",
        "                                                   device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHBddoOCR0Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 self_attention_layer, \n",
        "                 positionwise_feedforward_layer, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = self_attention_layer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = positionwise_feedforward_layer(hid_dim, \n",
        "                                                                       pf_dim, \n",
        "                                                                       dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, src len]\n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward, dropout, residual and layer norm\n",
        "        src = self.layer_norm(src + self.dropout(self.positionwise_feedforward(src)))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-GJPOErR5Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, seq len, seq len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, seq len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, seq len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6-BCYZWR7dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LudEW94vR9gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 decoder_layer, \n",
        "                 self_attention_layer, \n",
        "                 positionwise_feedforward_layer, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([decoder_layer(hid_dim, \n",
        "                                                   n_heads, \n",
        "                                                   pf_dim, \n",
        "                                                   self_attention_layer, \n",
        "                                                   positionwise_feedforward_layer, \n",
        "                                                   dropout, \n",
        "                                                   device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, trg len]\n",
        "        #src_mask = [batch size, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm1UxW4gSCts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 self_attention_layer, \n",
        "                 positionwise_feedforward_layer, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = self_attention_layer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = self_attention_layer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = positionwise_feedforward_layer(hid_dim, \n",
        "                                                                       pf_dim, \n",
        "                                                                       dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, trg len]\n",
        "        #src_mask = [batch size, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward, dropout, residual and layer norm\n",
        "        trg = self.layer_norm(trg + self.dropout(self.positionwise_feedforward(trg)))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si-zu6RPSE2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 trg_sos_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.trg_sos_idx = trg_sos_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, trg len, 1]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iQCJaWWSHuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 128\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 256\n",
        "DEC_PF_DIM = 256\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              EncoderLayer, \n",
        "              SelfAttentionLayer, \n",
        "              PositionwiseFeedforwardLayer, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DecoderLayer, \n",
        "              SelfAttentionLayer, \n",
        "              PositionwiseFeedforwardLayer, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKtAZxchSXUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "TRG_SOS_IDX = TRG.vocab.stoi[TRG.init_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, TRG_SOS_IDX, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofysc-4SwXTx",
        "colab_type": "code",
        "outputId": "9d1c2b3b-4df3-4357-dfdf-aebb080a3133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(8352, 128)\n",
              "    (pos_embedding): Embedding(1000, 128)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=128, out_features=256, bias=True)\n",
              "          (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=128, out_features=256, bias=True)\n",
              "          (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=128, out_features=256, bias=True)\n",
              "          (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(8183, 128)\n",
              "    (pos_embedding): Embedding(1000, 128)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=128, out_features=256, bias=True)\n",
              "          (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=128, out_features=256, bias=True)\n",
              "          (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): SelfAttentionLayer(\n",
              "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=128, out_features=256, bias=True)\n",
              "          (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=128, out_features=8183, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeCWMDYQSaBc",
        "colab_type": "code",
        "outputId": "5ff77c87-9c58-48ad-b98b-cc30884706ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,419,575 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOKyAIzeSgxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xavier initialization\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4OSXgefon1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6eHMPbBSlBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM75RiekSnr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDcE8qrESph8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    tqdm_meter = tqdm(iterator, unit=' batches', desc=f'[EPOCH {epoch}/{N_EPOCHS}]', leave=False, total=0)\n",
        "    \n",
        "    for i, batch in enumerate(tqdm_meter):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # update tqdm meter\n",
        "        tqdm_meter.set_postfix(loss=f'{loss.item():0.4f}', ppl=f'{math.exp(loss.item()):0.2f}')\n",
        "        tqdm_meter.update()\n",
        "\n",
        "    return epoch_loss / (i + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-4Slp1SuFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(tqdm(iterator, unit=' batches', leave=False, total=0)):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / (i + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur2Z6FfBSxxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4fQZndIS1XE",
        "colab_type": "code",
        "outputId": "d3ce7e5e-c25c-440f-f580-41640d837a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        print('\\n\\t--Found new best val loss')\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f'\\nEpoch: {epoch:02} (^_^) Time: {epoch_mins}m {epoch_secs}s', end='')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}', end='')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 2/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 01 (^_^) Time: 0m 43s\tTrain Loss: 7.696 | Train PPL: 2200.467\t Val. Loss: 7.216 |  Val. PPL: 1360.833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 3/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 02 (^_^) Time: 0m 42s\tTrain Loss: 6.651 | Train PPL: 773.763\t Val. Loss: 6.383 |  Val. PPL: 591.584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 4/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 03 (^_^) Time: 0m 43s\tTrain Loss: 6.019 | Train PPL: 411.247\t Val. Loss: 5.936 |  Val. PPL: 378.262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 5/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 04 (^_^) Time: 0m 42s\tTrain Loss: 5.576 | Train PPL: 264.047\t Val. Loss: 5.592 |  Val. PPL: 268.288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 6/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 05 (^_^) Time: 0m 42s\tTrain Loss: 5.224 | Train PPL: 185.748\t Val. Loss: 5.331 |  Val. PPL: 206.738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 7/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 06 (^_^) Time: 0m 42s\tTrain Loss: 4.942 | Train PPL: 140.065\t Val. Loss: 4.964 |  Val. PPL: 143.220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 8/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 07 (^_^) Time: 0m 43s\tTrain Loss: 4.685 | Train PPL: 108.322\t Val. Loss: 4.599 |  Val. PPL:  99.341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 9/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 08 (^_^) Time: 0m 43s\tTrain Loss: 4.467 | Train PPL:  87.133\t Val. Loss: 4.287 |  Val. PPL:  72.758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 10/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 09 (^_^) Time: 0m 43s\tTrain Loss: 4.279 | Train PPL:  72.163\t Val. Loss: 4.033 |  Val. PPL:  56.436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 11/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 10 (^_^) Time: 0m 43s\tTrain Loss: 4.104 | Train PPL:  60.569\t Val. Loss: 3.718 |  Val. PPL:  41.173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 12/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 11 (^_^) Time: 0m 43s\tTrain Loss: 3.946 | Train PPL:  51.723\t Val. Loss: 3.490 |  Val. PPL:  32.774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 13/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 12 (^_^) Time: 0m 43s\tTrain Loss: 3.797 | Train PPL:  44.560\t Val. Loss: 3.249 |  Val. PPL:  25.758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 14/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 13 (^_^) Time: 0m 43s\tTrain Loss: 3.669 | Train PPL:  39.206\t Val. Loss: 3.040 |  Val. PPL:  20.898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 15/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 14 (^_^) Time: 0m 43s\tTrain Loss: 3.545 | Train PPL:  34.645\t Val. Loss: 2.905 |  Val. PPL:  18.269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 16/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 15 (^_^) Time: 0m 43s\tTrain Loss: 3.427 | Train PPL:  30.795\t Val. Loss: 2.786 |  Val. PPL:  16.209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 17/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 16 (^_^) Time: 0m 43s\tTrain Loss: 3.320 | Train PPL:  27.650\t Val. Loss: 2.522 |  Val. PPL:  12.451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 18/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 17 (^_^) Time: 0m 43s\tTrain Loss: 3.219 | Train PPL:  25.013\t Val. Loss: 2.433 |  Val. PPL:  11.388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 19/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 18 (^_^) Time: 0m 42s\tTrain Loss: 3.128 | Train PPL:  22.826\t Val. Loss: 2.265 |  Val. PPL:   9.635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 20/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 19 (^_^) Time: 0m 43s\tTrain Loss: 3.041 | Train PPL:  20.921\t Val. Loss: 2.124 |  Val. PPL:   8.361\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 21/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 20 (^_^) Time: 0m 43s\tTrain Loss: 2.955 | Train PPL:  19.193\t Val. Loss: 1.974 |  Val. PPL:   7.198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 22/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 21 (^_^) Time: 0m 43s\tTrain Loss: 2.880 | Train PPL:  17.812\t Val. Loss: 1.897 |  Val. PPL:   6.665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 23/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 22 (^_^) Time: 0m 43s\tTrain Loss: 2.803 | Train PPL:  16.487\t Val. Loss: 1.783 |  Val. PPL:   5.948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 24/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 23 (^_^) Time: 0m 43s\tTrain Loss: 2.732 | Train PPL:  15.360\t Val. Loss: 1.687 |  Val. PPL:   5.406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 25/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 24 (^_^) Time: 0m 43s\tTrain Loss: 2.670 | Train PPL:  14.445\t Val. Loss: 1.628 |  Val. PPL:   5.092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 26/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 25 (^_^) Time: 0m 43s\tTrain Loss: 2.606 | Train PPL:  13.540\t Val. Loss: 1.542 |  Val. PPL:   4.674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 27/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 26 (^_^) Time: 0m 43s\tTrain Loss: 2.547 | Train PPL:  12.764\t Val. Loss: 1.459 |  Val. PPL:   4.302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 28/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 27 (^_^) Time: 0m 43s\tTrain Loss: 2.493 | Train PPL:  12.100\t Val. Loss: 1.372 |  Val. PPL:   3.945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 29/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 28 (^_^) Time: 0m 43s\tTrain Loss: 2.438 | Train PPL:  11.452\t Val. Loss: 1.324 |  Val. PPL:   3.759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 30/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 29 (^_^) Time: 0m 43s\tTrain Loss: 2.396 | Train PPL:  10.978\t Val. Loss: 1.263 |  Val. PPL:   3.535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 31/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 30 (^_^) Time: 0m 43s\tTrain Loss: 2.341 | Train PPL:  10.388\t Val. Loss: 1.185 |  Val. PPL:   3.270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 32/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 31 (^_^) Time: 0m 43s\tTrain Loss: 2.294 | Train PPL:   9.919\t Val. Loss: 1.135 |  Val. PPL:   3.111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 33/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 32 (^_^) Time: 0m 43s\tTrain Loss: 2.251 | Train PPL:   9.496\t Val. Loss: 1.106 |  Val. PPL:   3.023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 34/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 33 (^_^) Time: 0m 43s\tTrain Loss: 2.211 | Train PPL:   9.125\t Val. Loss: 1.046 |  Val. PPL:   2.847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 35/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 34 (^_^) Time: 0m 43s\tTrain Loss: 2.169 | Train PPL:   8.750\t Val. Loss: 0.976 |  Val. PPL:   2.653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 36/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 35 (^_^) Time: 0m 43s\tTrain Loss: 2.133 | Train PPL:   8.440\t Val. Loss: 0.984 |  Val. PPL:   2.676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 37/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 36 (^_^) Time: 0m 43s\tTrain Loss: 2.094 | Train PPL:   8.119\t Val. Loss: 0.900 |  Val. PPL:   2.459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 38/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 37 (^_^) Time: 0m 43s\tTrain Loss: 2.063 | Train PPL:   7.866\t Val. Loss: 0.852 |  Val. PPL:   2.345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 39/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 38 (^_^) Time: 0m 43s\tTrain Loss: 2.028 | Train PPL:   7.602\t Val. Loss: 0.835 |  Val. PPL:   2.305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 40/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 39 (^_^) Time: 0m 43s\tTrain Loss: 1.994 | Train PPL:   7.343\t Val. Loss: 0.799 |  Val. PPL:   2.224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 41/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 40 (^_^) Time: 0m 43s\tTrain Loss: 1.970 | Train PPL:   7.174\t Val. Loss: 0.795 |  Val. PPL:   2.214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 42/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 41 (^_^) Time: 0m 43s\tTrain Loss: 1.936 | Train PPL:   6.928\t Val. Loss: 0.752 |  Val. PPL:   2.122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 43/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 42 (^_^) Time: 0m 43s\tTrain Loss: 1.910 | Train PPL:   6.754\t Val. Loss: 0.760 |  Val. PPL:   2.138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 44/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 43 (^_^) Time: 0m 43s\tTrain Loss: 1.885 | Train PPL:   6.589\t Val. Loss: 0.725 |  Val. PPL:   2.064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 45/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 44 (^_^) Time: 0m 43s\tTrain Loss: 1.856 | Train PPL:   6.397\t Val. Loss: 0.712 |  Val. PPL:   2.038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 46/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 45 (^_^) Time: 0m 43s\tTrain Loss: 1.830 | Train PPL:   6.235\t Val. Loss: 0.686 |  Val. PPL:   1.987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 47/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 46 (^_^) Time: 0m 43s\tTrain Loss: 1.803 | Train PPL:   6.069\t Val. Loss: 0.686 |  Val. PPL:   1.986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 48/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 47 (^_^) Time: 0m 43s\tTrain Loss: 1.786 | Train PPL:   5.966\t Val. Loss: 0.630 |  Val. PPL:   1.877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 49/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 48 (^_^) Time: 0m 43s\tTrain Loss: 1.758 | Train PPL:   5.802\t Val. Loss: 0.618 |  Val. PPL:   1.856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 50/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 49 (^_^) Time: 0m 43s\tTrain Loss: 1.739 | Train PPL:   5.690\t Val. Loss: 0.607 |  Val. PPL:   1.836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 51/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 50 (^_^) Time: 0m 43s\tTrain Loss: 1.718 | Train PPL:   5.573\t Val. Loss: 0.569 |  Val. PPL:   1.766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 52/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 51 (^_^) Time: 0m 43s\tTrain Loss: 1.696 | Train PPL:   5.452\t Val. Loss: 0.560 |  Val. PPL:   1.751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 53/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 52 (^_^) Time: 0m 43s\tTrain Loss: 1.680 | Train PPL:   5.367\t Val. Loss: 0.535 |  Val. PPL:   1.708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 54/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 53 (^_^) Time: 0m 43s\tTrain Loss: 1.658 | Train PPL:   5.251\t Val. Loss: 0.524 |  Val. PPL:   1.688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 55/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 54 (^_^) Time: 0m 43s\tTrain Loss: 1.643 | Train PPL:   5.170\t Val. Loss: 0.497 |  Val. PPL:   1.643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 56/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 55 (^_^) Time: 0m 43s\tTrain Loss: 1.622 | Train PPL:   5.062\t Val. Loss: 0.472 |  Val. PPL:   1.603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 57/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 56 (^_^) Time: 0m 43s\tTrain Loss: 1.607 | Train PPL:   4.987\t Val. Loss: 0.468 |  Val. PPL:   1.596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 58/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 57 (^_^) Time: 0m 43s\tTrain Loss: 1.585 | Train PPL:   4.880\t Val. Loss: 0.468 |  Val. PPL:   1.597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 59/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 58 (^_^) Time: 0m 43s\tTrain Loss: 1.575 | Train PPL:   4.829\t Val. Loss: 0.467 |  Val. PPL:   1.595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 60/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 59 (^_^) Time: 0m 43s\tTrain Loss: 1.557 | Train PPL:   4.743\t Val. Loss: 0.429 |  Val. PPL:   1.536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 61/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 60 (^_^) Time: 0m 43s\tTrain Loss: 1.540 | Train PPL:   4.667\t Val. Loss: 0.414 |  Val. PPL:   1.513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 62/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 61 (^_^) Time: 0m 43s\tTrain Loss: 1.527 | Train PPL:   4.605\t Val. Loss: 0.407 |  Val. PPL:   1.503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 63/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 62 (^_^) Time: 0m 43s\tTrain Loss: 1.519 | Train PPL:   4.568\t Val. Loss: 0.422 |  Val. PPL:   1.526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 64/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 63 (^_^) Time: 0m 43s\tTrain Loss: 1.499 | Train PPL:   4.479\t Val. Loss: 0.397 |  Val. PPL:   1.487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 65/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 64 (^_^) Time: 0m 43s\tTrain Loss: 1.490 | Train PPL:   4.439\t Val. Loss: 0.375 |  Val. PPL:   1.456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 66/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 65 (^_^) Time: 0m 43s\tTrain Loss: 1.477 | Train PPL:   4.379\t Val. Loss: 0.372 |  Val. PPL:   1.451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 67/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 66 (^_^) Time: 0m 43s\tTrain Loss: 1.462 | Train PPL:   4.315\t Val. Loss: 0.368 |  Val. PPL:   1.445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 68/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 67 (^_^) Time: 0m 43s\tTrain Loss: 1.452 | Train PPL:   4.272\t Val. Loss: 0.366 |  Val. PPL:   1.442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 69/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 68 (^_^) Time: 0m 42s\tTrain Loss: 1.438 | Train PPL:   4.211\t Val. Loss: 0.356 |  Val. PPL:   1.427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 70/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 69 (^_^) Time: 0m 42s\tTrain Loss: 1.429 | Train PPL:   4.176\t Val. Loss: 0.348 |  Val. PPL:   1.416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 71/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 70 (^_^) Time: 0m 42s\tTrain Loss: 1.419 | Train PPL:   4.135\t Val. Loss: 0.346 |  Val. PPL:   1.414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 72/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 71 (^_^) Time: 0m 43s\tTrain Loss: 1.404 | Train PPL:   4.072\t Val. Loss: 0.341 |  Val. PPL:   1.406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 73/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 72 (^_^) Time: 0m 43s\tTrain Loss: 1.393 | Train PPL:   4.026\t Val. Loss: 0.351 |  Val. PPL:   1.420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 74/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 73 (^_^) Time: 0m 43s\tTrain Loss: 1.385 | Train PPL:   3.994\t Val. Loss: 0.318 |  Val. PPL:   1.374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 75/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 74 (^_^) Time: 0m 43s\tTrain Loss: 1.376 | Train PPL:   3.959\t Val. Loss: 0.311 |  Val. PPL:   1.365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 76/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 75 (^_^) Time: 0m 43s\tTrain Loss: 1.369 | Train PPL:   3.929\t Val. Loss: 0.308 |  Val. PPL:   1.361\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 77/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 76 (^_^) Time: 0m 43s\tTrain Loss: 1.358 | Train PPL:   3.889\t Val. Loss: 0.324 |  Val. PPL:   1.383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 78/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 77 (^_^) Time: 0m 43s\tTrain Loss: 1.347 | Train PPL:   3.844\t Val. Loss: 0.303 |  Val. PPL:   1.353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 79/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 78 (^_^) Time: 0m 43s\tTrain Loss: 1.336 | Train PPL:   3.802\t Val. Loss: 0.281 |  Val. PPL:   1.325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 80/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 79 (^_^) Time: 0m 43s\tTrain Loss: 1.327 | Train PPL:   3.769\t Val. Loss: 0.281 |  Val. PPL:   1.325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 81/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 80 (^_^) Time: 0m 43s\tTrain Loss: 1.322 | Train PPL:   3.750\t Val. Loss: 0.264 |  Val. PPL:   1.302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 82/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 81 (^_^) Time: 0m 43s\tTrain Loss: 1.305 | Train PPL:   3.689\t Val. Loss: 0.269 |  Val. PPL:   1.309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 83/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 82 (^_^) Time: 0m 43s\tTrain Loss: 1.300 | Train PPL:   3.668\t Val. Loss: 0.264 |  Val. PPL:   1.302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 84/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 83 (^_^) Time: 0m 43s\tTrain Loss: 1.291 | Train PPL:   3.637\t Val. Loss: 0.250 |  Val. PPL:   1.284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 85/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 84 (^_^) Time: 0m 43s\tTrain Loss: 1.285 | Train PPL:   3.615\t Val. Loss: 0.317 |  Val. PPL:   1.373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 86/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 85 (^_^) Time: 0m 43s\tTrain Loss: 1.280 | Train PPL:   3.597\t Val. Loss: 0.275 |  Val. PPL:   1.317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 87/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 86 (^_^) Time: 0m 43s\tTrain Loss: 1.272 | Train PPL:   3.569\t Val. Loss: 0.264 |  Val. PPL:   1.302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 88/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 87 (^_^) Time: 0m 43s\tTrain Loss: 1.262 | Train PPL:   3.534\t Val. Loss: 0.261 |  Val. PPL:   1.298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 89/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 88 (^_^) Time: 0m 43s\tTrain Loss: 1.254 | Train PPL:   3.505\t Val. Loss: 0.260 |  Val. PPL:   1.297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 90/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 89 (^_^) Time: 0m 43s\tTrain Loss: 1.252 | Train PPL:   3.496\t Val. Loss: 0.258 |  Val. PPL:   1.294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 91/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 90 (^_^) Time: 0m 43s\tTrain Loss: 1.240 | Train PPL:   3.456\t Val. Loss: 0.237 |  Val. PPL:   1.268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 92/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 91 (^_^) Time: 0m 43s\tTrain Loss: 1.234 | Train PPL:   3.437\t Val. Loss: 0.233 |  Val. PPL:   1.263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 93/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 92 (^_^) Time: 0m 43s\tTrain Loss: 1.227 | Train PPL:   3.411\t Val. Loss: 0.252 |  Val. PPL:   1.287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 94/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 93 (^_^) Time: 0m 43s\tTrain Loss: 1.226 | Train PPL:   3.407\t Val. Loss: 0.236 |  Val. PPL:   1.266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 95/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 94 (^_^) Time: 0m 43s\tTrain Loss: 1.218 | Train PPL:   3.381\t Val. Loss: 0.225 |  Val. PPL:   1.253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 96/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 95 (^_^) Time: 0m 43s\tTrain Loss: 1.208 | Train PPL:   3.348\t Val. Loss: 0.239 |  Val. PPL:   1.270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 97/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 96 (^_^) Time: 0m 43s\tTrain Loss: 1.202 | Train PPL:   3.328\t Val. Loss: 0.225 |  Val. PPL:   1.252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 98/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 97 (^_^) Time: 0m 43s\tTrain Loss: 1.196 | Train PPL:   3.306\t Val. Loss: 0.252 |  Val. PPL:   1.287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 99/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 98 (^_^) Time: 0m 42s\tTrain Loss: 1.197 | Train PPL:   3.311\t Val. Loss: 0.211 |  Val. PPL:   1.235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 100/100]: 0 batches [00:00, ? batches/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 99 (^_^) Time: 0m 43s\tTrain Loss: 1.183 | Train PPL:   3.265\t Val. Loss: 0.206 |  Val. PPL:   1.229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--Found new best val loss\n",
            "\n",
            "Epoch: 100 (^_^) Time: 0m 43s\tTrain Loss: 1.179 | Train PPL:   3.251\t Val. Loss: 0.202 |  Val. PPL:   1.224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcXp_EwymvH8",
        "colab_type": "code",
        "outputId": "45bf2ad6-d551-4b31-ec40-9ae18774c6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "# plot losses\n",
        "plt.figure(figsize=(5, 3), dpi=160)\n",
        "plt.plot(range(1, N_EPOCHS + 1), train_losses, color='tab:cyan', label='train')\n",
        "plt.plot(range(1, N_EPOCHS + 1), val_losses, color='tab:orange', label='val')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHPCAYAAABQjxzgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 800x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENZtE4qvS58z",
        "colab_type": "code",
        "outputId": "25e9d760-dc74-40d8-8b07-e886d398b605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 0.202 | Test PPL:   1.224 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvGlBN8qUlo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    tokens = [src_field.init_token] + sentence + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46fHNxhvDgLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbaef2c9-0a22-4ac8-db8e-72b469dc8260"
      },
      "source": [
        "!ls gdrive/My\\ Drive/EN-OD\\ translation"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  en-od  od-en  OR51_Ananta.ttf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntkZvD_Bh7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import font_manager as fm\n",
        "fpath = os.path.join('gdrive', 'My Drive', 'EN-OD translation', 'OR51_Ananta.ttf')\n",
        "prop = fm.FontProperties(fname=fpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh2GQ-imUqFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(1, n_heads+1):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols,i)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i-1].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='Oranges')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+sentence+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation, fontproperties=prop)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOJsMVoMUscy",
        "colab_type": "code",
        "outputId": "aa052a5c-b41b-4f0a-faa8-9f1d3e2b56ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "example_idx = 11\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['src']\n",
        "trg = vars(train_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['▁R', 'ights', '▁Man', 'ager', '▁for', '▁Music']\n",
            "trg = ['▁ସଙ୍ଗୀତ', '▁ପାଇଁ', '▁Right', 's', '▁Man', 'ag', 'er']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iLU20mWUu-j",
        "colab_type": "code",
        "outputId": "eea608cd-2726-4067-8d7b-66a297e57bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['▁ସଙ୍ଗୀତ', '▁ପାଇଁ', '▁Right', 's', '▁Man', 'ag', 'er', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYianCagU6yY",
        "colab_type": "code",
        "outputId": "c3a1a4dc-7ec5-48c9-bc94-7c67d859d662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 9601 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 9601 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAywAAAWPCAYAAABK1nDnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEiwAAEBbggUAAGhLsAAAAG39B0BAf7ORa6YWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x1800 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiVtZzTUZF9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "def ngrams_iterator(token_list, ngrams):\n",
        "    \"\"\"Return an iterator that yields the given tokens and their ngrams.\n",
        "    Arguments:\n",
        "        token_list: A list of tokens\n",
        "        ngrams: the number of ngrams.\n",
        "    Examples:\n",
        "        >>> token_list = ['here', 'we', 'are']\n",
        "        >>> list(ngrams_iterator(token_list, 2))\n",
        "        >>> ['here', 'here we', 'we', 'we are', 'are']\n",
        "    \"\"\"\n",
        "\n",
        "    def _get_ngrams(n):\n",
        "        return zip(*[token_list[i:] for i in range(n)])\n",
        "\n",
        "    for x in token_list:\n",
        "        yield x\n",
        "    for n in range(2, ngrams + 1):\n",
        "        for x in _get_ngrams(n):\n",
        "            yield ' '.join(x)\n",
        "\n",
        "\n",
        "def _compute_ngram_counter(tokens, max_n):\n",
        "    \"\"\" Create a Counter with a count of unique n-grams in the tokens list\n",
        "    Arguments:\n",
        "        tokens: a list of tokens (typically a string split on whitespaces)\n",
        "        max_n: the maximum order of n-gram wanted\n",
        "    Outputs:\n",
        "        output: a collections.Counter object with the unique n-grams and their\n",
        "            associated count\n",
        "    Examples:\n",
        "        >>> from torchtext.data.metrics import _compute_ngram_counter\n",
        "        >>> tokens = ['me', 'me', 'you']\n",
        "        >>> _compute_ngram_counter(tokens, 2)\n",
        "            Counter({('me',): 2,\n",
        "             ('you',): 1,\n",
        "             ('me', 'me'): 1,\n",
        "             ('me', 'you'): 1,\n",
        "             ('me', 'me', 'you'): 1})\n",
        "    \"\"\"\n",
        "    assert max_n > 0\n",
        "    ngrams_counter = collections.Counter(tuple(x.split(' '))\n",
        "                                         for x in ngrams_iterator(tokens, max_n))\n",
        "\n",
        "    return ngrams_counter\n",
        "\n",
        "\n",
        "def bleu_score(candidate_corpus, references_corpus, max_n=4, weights=[0.25] * 4):\n",
        "    \"\"\"Computes the BLEU score between a candidate translation corpus and a references\n",
        "    translation corpus. Based on https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "    Arguments:\n",
        "        candidate_corpus: an iterable of candidate translations. Each translation is an\n",
        "            iterable of tokens\n",
        "        references_corpus: an iterable of iterables of reference translations. Each\n",
        "            translation is an iterable of tokens\n",
        "        max_n: the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\n",
        "            bigrams and trigrams\n",
        "        weights: a list of weights used for each n-gram category (uniform by default)\n",
        "    Examples:\n",
        "        >>> from torchtext.data.metrics import bleu_score\n",
        "        >>> candidate_corpus = [['I', 'ate', 'the', 'apple'], ['I', 'did']]\n",
        "        >>> references_corpus = [[['I', 'ate', 'it'], ['I', 'ate', 'apples']],\n",
        "                [['I', 'did']]]\n",
        "        >>> bleu_score(candidate_corpus, references_corpus)\n",
        "            0.7598356856515925\n",
        "    \"\"\"\n",
        "\n",
        "    assert max_n == len(weights), 'Length of the \"weights\" list has be equal to max_n'\n",
        "    assert len(candidate_corpus) == len(references_corpus),\\\n",
        "        'The length of candidate and reference corpus should be the same'\n",
        "\n",
        "    clipped_counts = torch.zeros(max_n)\n",
        "    total_counts = torch.zeros(max_n)\n",
        "    weights = torch.tensor(weights)\n",
        "\n",
        "    candidate_len = 0.0\n",
        "    refs_len = 0.0\n",
        "\n",
        "    for (candidate, refs) in zip(candidate_corpus, references_corpus):\n",
        "        candidate_len += len(candidate)\n",
        "\n",
        "        # Get the length of the reference that's closest in length to the candidate\n",
        "        refs_len_list = [float(len(ref)) for ref in refs]\n",
        "        refs_len += min(refs_len_list, key=lambda x: abs(len(candidate) - x))\n",
        "\n",
        "        reference_counters = _compute_ngram_counter(refs[0], max_n)\n",
        "        for ref in refs[1:]:\n",
        "            reference_counters = reference_counters | _compute_ngram_counter(ref, max_n)\n",
        "\n",
        "        candidate_counter = _compute_ngram_counter(candidate, max_n)\n",
        "\n",
        "        clipped_counter = candidate_counter & reference_counters\n",
        "\n",
        "        for ngram in clipped_counter:\n",
        "            clipped_counts[len(ngram) - 1] += clipped_counter[ngram]\n",
        "\n",
        "        for ngram in candidate_counter:  # TODO: no need to loop through the whole counter\n",
        "            total_counts[len(ngram) - 1] += candidate_counter[ngram]\n",
        "\n",
        "    if min(clipped_counts) == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        pn = clipped_counts / total_counts\n",
        "        log_pn = weights * torch.log(pn)\n",
        "        score = torch.exp(sum(log_pn))\n",
        "\n",
        "        bp = math.exp(min(1 - refs_len / candidate_len, 0))\n",
        "\n",
        "        return bp * score.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZXwxQo1U_Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentences(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in tqdm(data, unit=' samples'):\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        trgs.append([trg])\n",
        "        pred_trgs.append(pred_trg)\n",
        "        \n",
        "    return trgs, pred_trgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey3P2WE9ZEu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f0258d3-cde6-4ace-b2ef-0e7f6c82c96e"
      },
      "source": [
        "labels, preds = translate_sentences(test_data, SRC, TRG, model, device)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3922/3922 [02:51<00:00, 22.93 samples/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMy39NT_bj3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sacrebleu\n",
        "import sacrebleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owtdQ9xInvhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outs = preds\n",
        "refs = labels\n",
        "outs_str = list(map(lambda l: sp_bpe_trg.decode_pieces(l), outs))\n",
        "refs_str = [list(map(lambda l: sp_bpe_trg.decode_pieces(l[0]), refs))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm4HHzyHfTLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a58d82c7-5fc9-4a13-d705-ca3a3cf7677a"
      },
      "source": [
        "print(f'BLEU score = {bleu_score(outs, refs)*100:.2f}')\n",
        "print(f'SACREBLEU score = {sacrebleu.corpus_bleu(outs_str, refs_str, force=False).score:.2f}')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 83.58\n",
            "SACREBLEU score = 81.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48J17jwTkRT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}